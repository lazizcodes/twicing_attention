Source code for our paper "Transformer Meets Twicing: Harnessing Unattended Residual Information" [ICLR 2025].

For language modeling on WikiText-103, use `twicing_attention_lang`. For vision tasks, use `twicing_attention_vision`.

If you want to cite our work, please use the following Bibtex item:
```
@inproceedings{
abdullaev2025transformer,
title={Transformer Meets Twicing: Harnessing Unattended Residual Information},
author={Laziz Abdullaev and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=16kG5aNleS}
}
```